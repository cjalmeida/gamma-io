# Configuration file used in tests

_workdir: !env IO_TEST_TMP
_project: !env IO_TEST_PROJECT_ROOT

# File-systems options
filesystems:
  # Options entry for 'test-bucket', you can have any name here
  test_bucket:
    # the regex to match against the location URL
    match: "s3://test-bucket/.*"

    # everything below here is returned as part of the options map
    endpoint_url: http://localhost:4566

datasets:
  _staging:
    use_staging: !env IO_TEST_STAGE|false
    location: !j2 "file://{{ c._workdir }}/stage/{layer}/{name}/"

  source:
    customers_1k:
      location: https://github.com/cjalmeida/gamma-io/raw/main/samples/customers-1000.zip
      format: csv
      args:
        compression: zip

    customers_1k_plain:
      location: https://github.com/cjalmeida/gamma-io/raw/main/samples/customers-1000.csv
      format: csv

    customers_1k_local:
      location: !j2 "file://{{c._project}}/samples/customers-1000.zip"
      format: csv
      args:
        compression: zip

    customers_1k_local_plain:
      location: !j2 "file://{{c._project}}/samples/customers-1000.csv"
      format: csv

  run:
    # this should be treated as a single file
    _dynamic:
      location: !j2 "file://{{ c._workdir }}/data/run/{name}.{ext}"
      format: parquet
      params:
        ext: parquet

  raw:
    # partitioned
    customers_parquet:
      location: !j2 "file://{{ c._workdir }}/data/customers_parquet"
      format: parquet
      partition_by: [l1, l2]
      args:
        compression: zstd

    # force as a single file
    customers_parquet_single:
      location: !j2 "file://{{ c._workdir }}/data/customers_parquet_single"
      is_file: true
      format: parquet
      args:
        compression: zstd

    # partitioned
    customers_feather:
      location: !j2 "file://{{ c._workdir }}/data/customers_feather"
      format: feather
      partition_by: [l1, l2]
      args:
        compression: zstd

    # treated as a single file via heuristics
    customers_csv_zip:
      location: !j2 "file://{{ c._workdir }}/data/customers.csv.zip"
      format: csv
      args:
        compression: zip

    # force as a single file
    customers_feather_single:
      location: !j2 "file://{{ c._workdir }}/data/customers_feather_single"
      is_file: true
      format: feather
      args:
        compression: zstd

    customers_csv_gz:
      location: !j2 "file://{{ c._workdir }}/data/customers.csv.gz"
      format: csv
      args:
        compression: gzip

    customers_csv:
      location: !j2 "file://{{ c._workdir }}/data/customers.csv"
      format: csv

    customers_excel:
      location: !j2 "file://{{ c._workdir }}/data/customers.xlsx"
      format: excel

    customers_s3:
      location: "s3://test-bucket/customers"
      format: parquet
      partition_by: [l1, l2]
      args:
        compression: snappy # for Spark compat

    customers_s3_copy:
      location: "s3://test-bucket/customers_copy"
      format: parquet
      partition_by: [l1, l2]
      args:
        compression: snappy # for Spark compat

    customers_sql_table:
      # format for sql connections is `sql:{sqlalchemy url}`
      # see https://docs.sqlalchemy.org/en/20/core/engines.html#sqlite
      location: !j2 "sql:sqlite:///{{ c._workdir }}/sql/data.db"

      # allow writing a full dataframe as table
      format: sql_table

      # read/write arguments. 'table_name' is mandatory for sql_table
      args:
        table_name: customers

      # SQLAlchemy create_engine conn arguments
      engine_args:
        echo: false

    customers_sql_query:
      location: !j2 "sql:sqlite:///{{ c._workdir }}/sql/data.db"
      format: sql_query
      args:
        sql: !j2 "file://{{ c._project }}/samples/customers.sql"
